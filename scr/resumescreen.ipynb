{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume screening\n",
    "This notebook goes through the process of screening a batch of resumes against a job specification, using NLP tools. The first step looks at named-entity recognition (NER) to extract key information from the resume. Next will be a look at the cosine similarity of each resume against the job spec, with the aim of filtering out the less relevent resumes. Third will be grouping the resumes together using LDA and finally using TF-IDF to attempt to rank the resumes for the given role. \n",
    "\n",
    "The resumes will then be uploaded to a vector database to simplify searching for appropriate candidates in the future.\n",
    "\n",
    "Future developments to be made in a recommendation/feedback system. The recruiter can rate (out of say 10) a candidate for a given role and then use a random forest machine learning model to predict ratings for other resumes. This will then be fed into the distance between the resume vector and job spec vector, with positive resumes moving closer and therefore being recommended more frequently. This would require a lot of human feedback initially to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/riz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/riz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load resumes\n",
    "The functions below load .pdf or .docx resumes. An error log file is created if any of the resume's aren't in either pdf or docx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from docx import Document\n",
    "import os\n",
    "import uuid\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(reader.numPages):\n",
    "            text += reader.getPage(page_num).extractText()\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.endswith('.pdf'):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        with open('../output/incorrect_format.txt', 'a') as log_file:\n",
    "            log_file.write(f'{file_path}\\n')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've taken a resume dataset from [Kaggle](https://www.kaggle.com/datasets/gauravduttakiit/resume-dataset) rather than loading a set of resumes individually. Loading the resumes also attaches a UUID to better help tracking down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# load resumes\n",
    "resume_text = pd.read_csv('../data/UpdatedResumeDataSet.csv')\n",
    "# assign uuid\n",
    "resume_text['resume_id'] = [str(uuid.uuid4()) for _ in range(len(resume_text))]\n",
    "# rename dataset columns\n",
    "resume_text.rename(columns={'Category': 'category', 'Resume': 'text'}, inplace=True)\n",
    "resume_text = resume_text[['resume_id', 'category', 'text']]\n",
    "'''\n",
    "\n",
    "resume_directory = \"../data/resume-dataset/data/data/INFORMATION-TECHNOLOGY\"  # set resume directory \n",
    "resume_text_dict = {}\n",
    "for filename in os.listdir(resume_directory):\n",
    "    file_path = os.path.join(resume_directory, filename)\n",
    "    resume_id = str(uuid.uuid4())  # Generate a unique ID\n",
    "    resume_text_dict[resume_id] = {\n",
    "        'filename': filename,\n",
    "        'text': extract_text(file_path)\n",
    "    }\n",
    "\n",
    "data = []\n",
    "for resume_id, info in resume_text_dict.items():\n",
    "    row = {'id': resume_id, 'filename': info['filename'], 'text': info['text']}\n",
    "    data.append(row)\n",
    "\n",
    "# set as dataframe\n",
    "resume_text = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398b70dd-2d80-4863-ada5-92de39fedc39</td>\n",
       "      <td>18176523.pdf</td>\n",
       "      <td>SENIOR INFORMATION TECHNOLOGY MANAGER\\nExecuti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2c614501-e010-44f4-8758-78e8e1db7104</td>\n",
       "      <td>25857360.pdf</td>\n",
       "      <td>STAFF ASSISTANT\\nProfessional Summary\\nHighly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d81c49f-759a-42db-9e66-ecd635cd27a1</td>\n",
       "      <td>39718499.pdf</td>\n",
       "      <td>ASSISTANT FOOTBALL COACH\\nSummary\\nEnthusiasti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29abdfe8-6538-49f5-82dd-f51507d85b53</td>\n",
       "      <td>40018190.pdf</td>\n",
       "      <td>IT SUPPORT TECHNICIAN\\nEducation\\nBachelor of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>edb32c28-7229-4c45-87d7-727bfdb3c74f</td>\n",
       "      <td>31243710.pdf</td>\n",
       "      <td>IT MANAGER\\nSummary\\nTen years of management e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id      filename  \\\n",
       "0  398b70dd-2d80-4863-ada5-92de39fedc39  18176523.pdf   \n",
       "1  2c614501-e010-44f4-8758-78e8e1db7104  25857360.pdf   \n",
       "2  7d81c49f-759a-42db-9e66-ecd635cd27a1  39718499.pdf   \n",
       "3  29abdfe8-6538-49f5-82dd-f51507d85b53  40018190.pdf   \n",
       "4  edb32c28-7229-4c45-87d7-727bfdb3c74f  31243710.pdf   \n",
       "\n",
       "                                                text  \n",
       "0  SENIOR INFORMATION TECHNOLOGY MANAGER\\nExecuti...  \n",
       "1  STAFF ASSISTANT\\nProfessional Summary\\nHighly ...  \n",
       "2  ASSISTANT FOOTBALL COACH\\nSummary\\nEnthusiasti...  \n",
       "3  IT SUPPORT TECHNICIAN\\nEducation\\nBachelor of ...  \n",
       "4  IT MANAGER\\nSummary\\nTen years of management e...  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also define some functions for preprocessing the resumes, using regex to attempt to extract email & phone numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text.lower().split() if word not in stop_words]\n",
    "\n",
    "#resume_text['email'] = resume_text['text'].apply(extract_email)\n",
    "#resume_text['phone_number'] = resume_text['text'].apply(extract_phone_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature extraction/NER\n",
    "\n",
    "We'll attempt to extract some key features from the resume and save them in the dataframe. Email and phone number should be reasonable straight forward using regex, but for the person's name, we'll attempt to use BERT for NER to detect 'person (PER)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_email(text):\n",
    "\n",
    "    pattern = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$'\n",
    "    match = re.findall(pattern, text)\n",
    "    return match[0] if match else None  # returns the first found email or None\n",
    "\n",
    "def extract_phone_number(text):\n",
    "    \n",
    "    # regex to match UK phone number but can be changed\n",
    "    pattern = r'^(((\\+44\\s?\\d{4}|\\(?0\\d{4}\\)?)\\s?\\d{3}\\s?\\d{3})|((\\+44\\s?\\d{3}|\\(?0\\d{3}\\)?)\\s?\\d{3}\\s?\\d{4})|((\\+44\\s?\\d{2}|\\(?0\\d{2}\\)?)\\s?\\d{4}\\s?\\d{4}))(\\s?\\#(\\d{4}|\\d{3}))?$'\n",
    "    match = re.findall(pattern, text)\n",
    "    return match[0] if match else None  # returns the first found phone number or None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>email</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>padding_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398b70dd-2d80-4863-ada5-92de39fedc39</td>\n",
       "      <td>18176523.pdf</td>\n",
       "      <td>SENIOR INFORMATION TECHNOLOGY MANAGER\\nExecuti...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[senior, information, technology, manager, exe...</td>\n",
       "      <td>[python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2c614501-e010-44f4-8758-78e8e1db7104</td>\n",
       "      <td>25857360.pdf</td>\n",
       "      <td>STAFF ASSISTANT\\nProfessional Summary\\nHighly ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[staff, assistant, professional, summary, high...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7d81c49f-759a-42db-9e66-ecd635cd27a1</td>\n",
       "      <td>39718499.pdf</td>\n",
       "      <td>ASSISTANT FOOTBALL COACH\\nSummary\\nEnthusiasti...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[assistant, football, coach, summary, enthusia...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29abdfe8-6538-49f5-82dd-f51507d85b53</td>\n",
       "      <td>40018190.pdf</td>\n",
       "      <td>IT SUPPORT TECHNICIAN\\nEducation\\nBachelor of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[support, technician, education, bachelor, sci...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>edb32c28-7229-4c45-87d7-727bfdb3c74f</td>\n",
       "      <td>31243710.pdf</td>\n",
       "      <td>IT MANAGER\\nSummary\\nTen years of management e...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[manager, summary, ten, year, management, expe...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id      filename  \\\n",
       "0  398b70dd-2d80-4863-ada5-92de39fedc39  18176523.pdf   \n",
       "1  2c614501-e010-44f4-8758-78e8e1db7104  25857360.pdf   \n",
       "2  7d81c49f-759a-42db-9e66-ecd635cd27a1  39718499.pdf   \n",
       "3  29abdfe8-6538-49f5-82dd-f51507d85b53  40018190.pdf   \n",
       "4  edb32c28-7229-4c45-87d7-727bfdb3c74f  31243710.pdf   \n",
       "\n",
       "                                                text email phone_number  \\\n",
       "0  SENIOR INFORMATION TECHNOLOGY MANAGER\\nExecuti...  None         None   \n",
       "1  STAFF ASSISTANT\\nProfessional Summary\\nHighly ...  None         None   \n",
       "2  ASSISTANT FOOTBALL COACH\\nSummary\\nEnthusiasti...  None         None   \n",
       "3  IT SUPPORT TECHNICIAN\\nEducation\\nBachelor of ...  None         None   \n",
       "4  IT MANAGER\\nSummary\\nTen years of management e...  None         None   \n",
       "\n",
       "                                          clean_text padding_words  \n",
       "0  [senior, information, technology, manager, exe...      [python]  \n",
       "1  [staff, assistant, professional, summary, high...            []  \n",
       "2  [assistant, football, coach, summary, enthusia...            []  \n",
       "3  [support, technician, education, bachelor, sci...            []  \n",
       "4  [manager, summary, ten, year, management, expe...            []  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_text['email'] = resume_text['text'].apply(extract_email)\n",
    "resume_text['phone_number'] = resume_text['text'].apply(extract_phone_number)\n",
    "resume_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to extract details using both spaCy and bert to perform named entity recognition (NER). Bert is likely to be less useful given it's only been trained to recognise location (LOC), organisations (ORG), person (PER) and miscellaneous (MISC).\n",
    "\n",
    "The spaCy pipeline will include a list of skills taken from [jobzilla skills dataset](https://github.com/kingabzpro/jobzilla_ai/blob/main/jz_skill_patterns.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text.to_csv('test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# LspaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'entity_ruler']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" skill_pattern_path = \"../data/jz_skill_patterns.jsonl\"\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "nlp.pipe_names\n",
    "\n",
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract named entities using bert ner\n",
    "def extract_entities_bert(resume_text):\n",
    "    entities = ner_pipeline(resume_text)\n",
    "    # Further processing to extract specific entities like name, education, etc.\n",
    "    # This might require custom logic based on the structure of your resumes\n",
    "    return entities\n",
    "\n",
    "# Function to extract named entities using spaCy\n",
    "def extract_entities_spacy(resume_text):\n",
    "    doc = nlp(resume_text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Category                                     Entities_spacy  \\\n",
      "0    Data Science  [(Sql, PERSON), (Java, PERSON), (JavaScript/JQ...   \n",
      "1    Data Science  [(May 2013 to, DATE), (May 2017, DATE), (Expri...   \n",
      "2    Data Science  [(Control System Design, ORG), (Web Developmen...   \n",
      "3    Data Science  [(Tableau, GPE), (SAP HANA SQL, ORG), (SAP HAN...   \n",
      "4    Data Science  [(MCA, ORG), (YMCAUST, ORG), (Faridabad, GPE),...   \n",
      "..            ...                                                ...   \n",
      "957       Testing  [(MS, GPE), (Basic Excel, PRODUCT), (Loyalty &...   \n",
      "958       Testing  [(Team Player, ORG), (DECLARATION, PERSON), (J...   \n",
      "959       Testing  [(Eagerness, NORP), (Competitive, ORG), (Janua...   \n",
      "960       Testing  [(SKILLS & SOFTWARE, ORG), (MS-Power Point, OR...   \n",
      "961       Testing  [(Skill Set OS, PERSON), (Windows XP/7/8/8.1/1...   \n",
      "\n",
      "                                         Entities_bert  \n",
      "0    [{'entity': 'I-MISC', 'score': 0.57808983, 'in...  \n",
      "1    [{'entity': 'I-ORG', 'score': 0.9233876, 'inde...  \n",
      "2    [{'entity': 'I-MISC', 'score': 0.4734587, 'ind...  \n",
      "3    [{'entity': 'I-ORG', 'score': 0.62514687, 'ind...  \n",
      "4    [{'entity': 'I-ORG', 'score': 0.9732521, 'inde...  \n",
      "..                                                 ...  \n",
      "957  [{'entity': 'I-ORG', 'score': 0.71640754, 'ind...  \n",
      "958  [{'entity': 'I-LOC', 'score': 0.8010248, 'inde...  \n",
      "959  [{'entity': 'I-ORG', 'score': 0.5006158, 'inde...  \n",
      "960  [{'entity': 'I-ORG', 'score': 0.94152266, 'ind...  \n",
      "961  [{'entity': 'I-MISC', 'score': 0.9764082, 'ind...  \n",
      "\n",
      "[962 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "resume_text['Entities_spacy'] = resume_text['Resume'].apply(extract_entities_spacy)\n",
    "resume_text['Entities_bert'] = resume_text['Resume'].apply(extract_entities_bert)\n",
    "\n",
    "# Display the new DataFrame with extracted entities\n",
    "print(resume_text[['Category', 'Entities_spacy', 'Entities_bert']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = resume_text.iloc[:5]\n",
    "\n",
    "# Initialize an empty list to store the transformed data\n",
    "transformed_data = []\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in selected_rows.iterrows():\n",
    "    # Iterate over each entity in the row\n",
    "    for entity in row['Entities_bert']:\n",
    "        # Append the entity data to the transformed_data list\n",
    "        transformed_data.append({\n",
    "            'Row': index,\n",
    "            'Entity': entity['entity'],\n",
    "            'Score': entity['score'],\n",
    "            'Index': entity['index']\n",
    "            # Add more fields here as needed\n",
    "        })\n",
    "\n",
    "# Convert the transformed data into a DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Row  Entity     Score  Index\n",
      "0      0  I-MISC  0.578090      6\n",
      "1      0   I-ORG  0.379557     33\n",
      "2      0  I-MISC  0.896526     37\n",
      "3      0  I-MISC  0.978586     39\n",
      "4      0  I-MISC  0.774978     40\n",
      "..   ...     ...       ...    ...\n",
      "290    4   I-ORG  0.427012     16\n",
      "291    4  I-MISC  0.964054     60\n",
      "292    4  I-MISC  0.971787     73\n",
      "293    4   I-ORG  0.769654    106\n",
      "294    4   I-ORG  0.793049    108\n",
      "\n",
      "[295 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cosine similarity\n",
    "Look at cosine similarity between resume and job spec, but first try to highlight any resume that has attempted to use word padding to attempt to manipulate any word based filtering system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text for both specs and resumes\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text.lower().split() if word not in stop_words]\n",
    "\n",
    "resume_text['clean_text'] = resume_text['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, filename, text, email, phone_number, clean_text, padding_words]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def detect_word_padding(text, threshold=0.1):\n",
    "    #words = text.split()\n",
    "    word_counts = Counter(text)\n",
    "    total_words = len(text)\n",
    "    \n",
    "    padding_words = []\n",
    "    for word, count in word_counts.items():\n",
    "        if count / total_words > threshold:\n",
    "            padding_words.append(word)\n",
    "\n",
    "    return padding_words\n",
    "\n",
    "resume_text['padding_words'] = resume_text['clean_text'].apply(detect_word_padding)\n",
    "padded_resumes = resume_text[resume_text['padding_words'].apply(lambda x: len(x) > 1)]\n",
    "\n",
    "print(padded_resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LDA Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary and Corpus for LDA\n",
    "id2word = corpora.Dictionary(processed_resumes)\n",
    "corpus = [id2word.doc2bow(text) for text in processed_resumes]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10, # Adjust the number of topics\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [skill, *, programming, languages:, python, (p...\n",
      "1      [education, detail, may, 2013, may, 2017, b.e,...\n",
      "2      [area, interest, deep, learning,, control, sys...\n",
      "3      [skill, â¢, r, â¢, python, â¢, sap, hana, â...\n",
      "4      [education, detail, mca, ymcaust,, faridabad,,...\n",
      "                             ...                        \n",
      "957    [computer, skills:, â¢, proficient, m, office...\n",
      "958    [â, willingness, accept, challenges., â, p...\n",
      "959    [personal, skill, â¢, quick, learner,, â¢, e...\n",
      "960    [computer, skill, &, software, knowledge, ms-p...\n",
      "961    [skill, set, o, window, xp/7/8/8.1/10, databas...\n",
      "Name: Resume, Length: 962, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. TF-IDF for keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Import to vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

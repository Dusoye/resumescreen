{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume screening\n",
    "This notebook goes through the process of screening a batch of resumes against a job specification, using NLP tools. The first step looks at named-entity recognition (NER) to extract key information from the resume. Next will be a look at the cosine similarity of each resume against the job spec, with the aim of filtering out the less relevent resumes. Third will be grouping the resumes together using LDA and finally using TF-IDF to attempt to rank the resumes for the given role. \n",
    "\n",
    "The resumes will then be uploaded to a vector database to simplify searching for appropriate candidates in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/riz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/riz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_text = pd.read_csv('../data/UpdatedResumeDataSet.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NER implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained bert model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Load pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'entity_ruler']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "skill_pattern_path = \"../data/jz_skill_patterns.jsonl\"\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "nlp.pipe_names\n",
    "\n",
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract named entities using bert ner\n",
    "def extract_entities_bert(resume_text):\n",
    "    entities = ner_pipeline(resume_text)\n",
    "    # Further processing to extract specific entities like name, education, etc.\n",
    "    # This might require custom logic based on the structure of your resumes\n",
    "    return entities\n",
    "\n",
    "# Function to extract named entities using spaCy\n",
    "def extract_entities_spacy(resume_text):\n",
    "    doc = nlp(resume_text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Category                                     Entities_spacy  \\\n",
      "0    Data Science  [(Sql, PERSON), (Java, PERSON), (JavaScript/JQ...   \n",
      "1    Data Science  [(May 2013 to, DATE), (May 2017, DATE), (Expri...   \n",
      "2    Data Science  [(Control System Design, ORG), (Web Developmen...   \n",
      "3    Data Science  [(Tableau, GPE), (SAP HANA SQL, ORG), (SAP HAN...   \n",
      "4    Data Science  [(MCA, ORG), (YMCAUST, ORG), (Faridabad, GPE),...   \n",
      "..            ...                                                ...   \n",
      "957       Testing  [(MS, GPE), (Basic Excel, PRODUCT), (Loyalty &...   \n",
      "958       Testing  [(Team Player, ORG), (DECLARATION, PERSON), (J...   \n",
      "959       Testing  [(Eagerness, NORP), (Competitive, ORG), (Janua...   \n",
      "960       Testing  [(SKILLS & SOFTWARE, ORG), (MS-Power Point, OR...   \n",
      "961       Testing  [(Skill Set OS, PERSON), (Windows XP/7/8/8.1/1...   \n",
      "\n",
      "                                         Entities_bert  \n",
      "0    [{'entity': 'I-MISC', 'score': 0.57808983, 'in...  \n",
      "1    [{'entity': 'I-ORG', 'score': 0.9233876, 'inde...  \n",
      "2    [{'entity': 'I-MISC', 'score': 0.4734587, 'ind...  \n",
      "3    [{'entity': 'I-ORG', 'score': 0.62514687, 'ind...  \n",
      "4    [{'entity': 'I-ORG', 'score': 0.9732521, 'inde...  \n",
      "..                                                 ...  \n",
      "957  [{'entity': 'I-ORG', 'score': 0.71640754, 'ind...  \n",
      "958  [{'entity': 'I-LOC', 'score': 0.8010248, 'inde...  \n",
      "959  [{'entity': 'I-ORG', 'score': 0.5006158, 'inde...  \n",
      "960  [{'entity': 'I-ORG', 'score': 0.94152266, 'ind...  \n",
      "961  [{'entity': 'I-MISC', 'score': 0.9764082, 'ind...  \n",
      "\n",
      "[962 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "resume_text['Entities_spacy'] = resume_text['Resume'].apply(extract_entities_spacy)\n",
    "resume_text['Entities_bert'] = resume_text['Resume'].apply(extract_entities_bert)\n",
    "\n",
    "# Display the new DataFrame with extracted entities\n",
    "print(resume_text[['Category', 'Entities_spacy', 'Entities_bert']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = resume_text.iloc[:5]\n",
    "\n",
    "# Initialize an empty list to store the transformed data\n",
    "transformed_data = []\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in selected_rows.iterrows():\n",
    "    # Iterate over each entity in the row\n",
    "    for entity in row['Entities_bert']:\n",
    "        # Append the entity data to the transformed_data list\n",
    "        transformed_data.append({\n",
    "            'Row': index,\n",
    "            'Entity': entity['entity'],\n",
    "            'Score': entity['score'],\n",
    "            'Index': entity['index']\n",
    "            # Add more fields here as needed\n",
    "        })\n",
    "\n",
    "# Convert the transformed data into a DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Row  Entity     Score  Index\n",
      "0      0  I-MISC  0.578090      6\n",
      "1      0   I-ORG  0.379557     33\n",
      "2      0  I-MISC  0.896526     37\n",
      "3      0  I-MISC  0.978586     39\n",
      "4      0  I-MISC  0.774978     40\n",
      "..   ...     ...       ...    ...\n",
      "290    4   I-ORG  0.427012     16\n",
      "291    4  I-MISC  0.964054     60\n",
      "292    4  I-MISC  0.971787     73\n",
      "293    4   I-ORG  0.769654    106\n",
      "294    4   I-ORG  0.793049    108\n",
      "\n",
      "[295 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cosine similarity\n",
    "Look at cosine similarity between resume and job spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text for both specs and resumes\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text.lower().split() if word not in stop_words]\n",
    "\n",
    "processed_resumes = resume_text['Resume'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LDA Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary and Corpus for LDA\n",
    "id2word = corpora.Dictionary(processed_resumes)\n",
    "corpus = [id2word.doc2bow(text) for text in processed_resumes]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10, # Adjust the number of topics\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [skill, *, programming, languages:, python, (p...\n",
      "1      [education, detail, may, 2013, may, 2017, b.e,...\n",
      "2      [area, interest, deep, learning,, control, sys...\n",
      "3      [skill, â¢, r, â¢, python, â¢, sap, hana, â...\n",
      "4      [education, detail, mca, ymcaust,, faridabad,,...\n",
      "                             ...                        \n",
      "957    [computer, skills:, â¢, proficient, m, office...\n",
      "958    [â, willingness, accept, challenges., â, p...\n",
      "959    [personal, skill, â¢, quick, learner,, â¢, e...\n",
      "960    [computer, skill, &, software, knowledge, ms-p...\n",
      "961    [skill, set, o, window, xp/7/8/8.1/10, databas...\n",
      "Name: Resume, Length: 962, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TF-IDF for keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Import to vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

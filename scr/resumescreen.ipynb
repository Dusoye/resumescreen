{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume screening\n",
    "This notebook goes through the process of screening a batch of resumes against a job specification, using NLP tools. The first step looks at named-entity recognition (NER) to extract key information from the resume. Next will be a look at the cosine similarity of each resume against the job spec, with the aim of filtering out the less relevent resumes. Third will be grouping the resumes together using LDA and finally using TF-IDF to attempt to rank the resumes for the given role. \n",
    "\n",
    "The resumes will then be uploaded to a vector database to simplify searching for appropriate candidates in the future.\n",
    "\n",
    "Future developments to be made in a recommendation/feedback system. The recruiter can rate (out of say 10) a candidate for a given role and then use a random forest machine learning model to predict ratings for other resumes. This will then be fed into the distance between the resume vector and job spec vector, with positive resumes moving closer and therefore being recommended more frequently. This would require a lot of human feedback initially to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/riz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/riz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load resumes\n",
    "The functions below load .pdf or .docx resumes. An error log file is created if any of the resume's aren't in either pdf or docx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from docx import Document\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfFileReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(reader.numPages):\n",
    "            text += reader.getPage(page_num).extractText()\n",
    "        return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.endswith('.pdf'):\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif file_path.endswith('.docx'):\n",
    "        return extract_text_from_docx(file_path)\n",
    "    else:\n",
    "        with open('../output/incorrect_format.txt', 'a') as log_file:\n",
    "            log_file.write(f'{file_path}\\n')\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've taken a resume dataset from [Kaggle](https://www.kaggle.com/datasets/gauravduttakiit/resume-dataset) rather than loading a set of resumes individually. Loading the resumes also attaches a UUID to better help tracking down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nresume_directory = \"\" # set resume directory \\nresumes_data = {}\\nfor filename in os.listdir(resume_directory):\\n    file_path = os.path.join(resume_directory, filename)\\n    resume_id = str(uuid.uuid4())  # Generate a unique ID\\n    resumes_data[resume_id] = {\\n        \\'filename\\': filename,\\n        \\'text\\': extract_text(file_path)\\n    }\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load resumes\n",
    "resume_text = pd.read_csv('../data/UpdatedResumeDataSet.csv')\n",
    "# assign uuid\n",
    "resume_text['resume_id'] = [str(uuid.uuid4()) for _ in range(len(resume_text))]\n",
    "# rename dataset columns\n",
    "resume_text.rename(columns={'Category': 'category', 'Resume': 'text'}, inplace=True)\n",
    "resume_text = resume_text[['resume_id', 'category', 'text']]\n",
    "\n",
    "'''\n",
    "resume_directory = \"../data/\" # set resume directory \n",
    "resume_text = {}\n",
    "for filename in os.listdir(resume_directory):\n",
    "    file_path = os.path.join(resume_directory, filename)\n",
    "    resume_id = str(uuid.uuid4())  # Generate a unique ID\n",
    "    resume_text[resume_id] = {\n",
    "        'filename': filename,\n",
    "        'text': extract_text(file_path)\n",
    "    }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0eaba668-3e28-42e2-a7ed-52ef781ebcc0</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74d6eaca-caf2-40e6-abf2-32ee26da6fb3</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bac86ce5-5407-4bd3-82f0-f8b9626d7180</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e20eb17f-92ea-4490-ac61-51f666aefe0a</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2b620507-22d8-4813-abd1-cd3d3ecbe482</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   uuid      category  \\\n",
       "0  0eaba668-3e28-42e2-a7ed-52ef781ebcc0  Data Science   \n",
       "1  74d6eaca-caf2-40e6-abf2-32ee26da6fb3  Data Science   \n",
       "2  bac86ce5-5407-4bd3-82f0-f8b9626d7180  Data Science   \n",
       "3  e20eb17f-92ea-4490-ac61-51f666aefe0a  Data Science   \n",
       "4  2b620507-22d8-4813-abd1-cd3d3ecbe482  Data Science   \n",
       "\n",
       "                                                text  \n",
       "0  Skills * Programming Languages: Python (pandas...  \n",
       "1  Education Details \\r\\nMay 2013 to May 2017 B.E...  \n",
       "2  Areas of Interest Deep Learning, Control Syste...  \n",
       "3  Skills â¢ R â¢ Python â¢ SAP HANA â¢ Table...  \n",
       "4  Education Details \\r\\n MCA   YMCAUST,  Faridab...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. NER implementation \n",
    "Attempt to extract details using both spaCy and bert to perform named entity recognition (NER). Bert is likely to be less useful given it's only been trained to recognise location (LOC), organisations (ORG), person (PER) and miscellaneous (MISC).\n",
    "\n",
    "The spaCy pipeline will include a list of skills taken from [jobzilla skills dataset](https://github.com/kingabzpro/jobzilla_ai/blob/main/jz_skill_patterns.jsonl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained bert model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "# Load pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'entity_ruler']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" skill_pattern_path = \"../data/jz_skill_patterns.jsonl\"\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(skill_pattern_path)\n",
    "nlp.pipe_names\n",
    "\n",
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def unique_skills(x):\n",
    "    return list(set(x)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract named entities using bert ner\n",
    "def extract_entities_bert(resume_text):\n",
    "    entities = ner_pipeline(resume_text)\n",
    "    # Further processing to extract specific entities like name, education, etc.\n",
    "    # This might require custom logic based on the structure of your resumes\n",
    "    return entities\n",
    "\n",
    "# Function to extract named entities using spaCy\n",
    "def extract_entities_spacy(resume_text):\n",
    "    doc = nlp(resume_text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Category                                     Entities_spacy  \\\n",
      "0    Data Science  [(Sql, PERSON), (Java, PERSON), (JavaScript/JQ...   \n",
      "1    Data Science  [(May 2013 to, DATE), (May 2017, DATE), (Expri...   \n",
      "2    Data Science  [(Control System Design, ORG), (Web Developmen...   \n",
      "3    Data Science  [(Tableau, GPE), (SAP HANA SQL, ORG), (SAP HAN...   \n",
      "4    Data Science  [(MCA, ORG), (YMCAUST, ORG), (Faridabad, GPE),...   \n",
      "..            ...                                                ...   \n",
      "957       Testing  [(MS, GPE), (Basic Excel, PRODUCT), (Loyalty &...   \n",
      "958       Testing  [(Team Player, ORG), (DECLARATION, PERSON), (J...   \n",
      "959       Testing  [(Eagerness, NORP), (Competitive, ORG), (Janua...   \n",
      "960       Testing  [(SKILLS & SOFTWARE, ORG), (MS-Power Point, OR...   \n",
      "961       Testing  [(Skill Set OS, PERSON), (Windows XP/7/8/8.1/1...   \n",
      "\n",
      "                                         Entities_bert  \n",
      "0    [{'entity': 'I-MISC', 'score': 0.57808983, 'in...  \n",
      "1    [{'entity': 'I-ORG', 'score': 0.9233876, 'inde...  \n",
      "2    [{'entity': 'I-MISC', 'score': 0.4734587, 'ind...  \n",
      "3    [{'entity': 'I-ORG', 'score': 0.62514687, 'ind...  \n",
      "4    [{'entity': 'I-ORG', 'score': 0.9732521, 'inde...  \n",
      "..                                                 ...  \n",
      "957  [{'entity': 'I-ORG', 'score': 0.71640754, 'ind...  \n",
      "958  [{'entity': 'I-LOC', 'score': 0.8010248, 'inde...  \n",
      "959  [{'entity': 'I-ORG', 'score': 0.5006158, 'inde...  \n",
      "960  [{'entity': 'I-ORG', 'score': 0.94152266, 'ind...  \n",
      "961  [{'entity': 'I-MISC', 'score': 0.9764082, 'ind...  \n",
      "\n",
      "[962 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "resume_text['Entities_spacy'] = resume_text['Resume'].apply(extract_entities_spacy)\n",
    "resume_text['Entities_bert'] = resume_text['Resume'].apply(extract_entities_bert)\n",
    "\n",
    "# Display the new DataFrame with extracted entities\n",
    "print(resume_text[['Category', 'Entities_spacy', 'Entities_bert']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_rows = resume_text.iloc[:5]\n",
    "\n",
    "# Initialize an empty list to store the transformed data\n",
    "transformed_data = []\n",
    "\n",
    "# Iterate over each row\n",
    "for index, row in selected_rows.iterrows():\n",
    "    # Iterate over each entity in the row\n",
    "    for entity in row['Entities_bert']:\n",
    "        # Append the entity data to the transformed_data list\n",
    "        transformed_data.append({\n",
    "            'Row': index,\n",
    "            'Entity': entity['entity'],\n",
    "            'Score': entity['score'],\n",
    "            'Index': entity['index']\n",
    "            # Add more fields here as needed\n",
    "        })\n",
    "\n",
    "# Convert the transformed data into a DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Row  Entity     Score  Index\n",
      "0      0  I-MISC  0.578090      6\n",
      "1      0   I-ORG  0.379557     33\n",
      "2      0  I-MISC  0.896526     37\n",
      "3      0  I-MISC  0.978586     39\n",
      "4      0  I-MISC  0.774978     40\n",
      "..   ...     ...       ...    ...\n",
      "290    4   I-ORG  0.427012     16\n",
      "291    4  I-MISC  0.964054     60\n",
      "292    4  I-MISC  0.971787     73\n",
      "293    4   I-ORG  0.769654    106\n",
      "294    4   I-ORG  0.793049    108\n",
      "\n",
      "[295 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(transformed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cosine similarity\n",
    "Look at cosine similarity between resume and job spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess text for both specs and resumes\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text.lower().split() if word not in stop_words]\n",
    "\n",
    "processed_resumes = resume_text['Resume'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LDA Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary and Corpus for LDA\n",
    "id2word = corpora.Dictionary(processed_resumes)\n",
    "corpus = [id2word.doc2bow(text) for text in processed_resumes]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10, # Adjust the number of topics\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [skill, *, programming, languages:, python, (p...\n",
      "1      [education, detail, may, 2013, may, 2017, b.e,...\n",
      "2      [area, interest, deep, learning,, control, sys...\n",
      "3      [skill, â¢, r, â¢, python, â¢, sap, hana, â...\n",
      "4      [education, detail, mca, ymcaust,, faridabad,,...\n",
      "                             ...                        \n",
      "957    [computer, skills:, â¢, proficient, m, office...\n",
      "958    [â, willingness, accept, challenges., â, p...\n",
      "959    [personal, skill, â¢, quick, learner,, â¢, e...\n",
      "960    [computer, skill, &, software, knowledge, ms-p...\n",
      "961    [skill, set, o, window, xp/7/8/8.1/10, databas...\n",
      "Name: Resume, Length: 962, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. TF-IDF for keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Import to vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
